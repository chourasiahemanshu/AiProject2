# AiProject2
Ai Project: Creating Naive Bayes classifier to check if the mail is spam or not


Preprocessing the data:
1. Everything apart from letters is excluded
2. Multiple spaces are replaced by single space
3. String is converted to lower case
Creating modal:
1.	A modal is created which includes the word and the count of the words to be spam and ham in all the training datasets.
2.	Conditional Probability is calculated by dividing the count of each word by the total of each class and is stored. It is smoothed a little to get better result.
3.	Class probability is calculated by formula : documents of particular class/total number of documents.
4.	These values are printed in file modal.txt in alphabetic order .
Testing Modal:
1.	Files to be tested are imported.
2.	The conditional probability of the file is calculated based on the modal for each word
3.	Based on that, it is classified as spam or ham which ever has higher score
4.	The result is printed in a file result.txt

Analysis:

True Positive : 394

False Positive :  64

True Negative :  336

False Negative :  6

Accuracy :  0.9125

Precision :  0.8602620087336245

Recall :  0.985

F1_score :  0.9184149184149185

 
The above Confusion Matric was generated by using python library matplotlib.
Based on the values that we calculated from the testing we got our counts required to calculate the Accuracy , precision , recall and f1 measure using the following formulas

accuracy = TP+TN / TP+FP+FN+TN

precision = TP/ TP+FP

recall = TP / TP+FN

F1 Score = 2*(Recall * Precision) / (Recall + Precision)

•	Accuracy works best if false positives and false negatives have similar cost. The accuracy of the model is 91.25%. 

•	Precision is a good measure to determine when the costs of False Positive is high. In our case, email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model. In our case it is 86.02% 

•	Recall (Sensitivity):  Recall is the ratio of correctly predicted positive observations to the all observations in actual class – yes. We have got recall of 0.985 which is good for this model as it’s above 0.5.

•	F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives). In our case it 91.84% which is good.
